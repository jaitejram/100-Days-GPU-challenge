{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfDR7f0WNSQu",
        "outputId": "30aef80f-62a0-4a4f-e088-a804ae6fe509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sgu.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile sgu.cu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sgu.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <curand_kernel.h>\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// CUDA kernel to compute predictions and squared loss\n",
        "__global__ void compute_loss(float* X, float* y, float* W, float* b, float* loss, float* y_pred, int N, int D, int epoch) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < N) {\n",
        "        float y_pred_val = 0.0f;\n",
        "        for (int i = 0; i < D; i++) {\n",
        "            y_pred_val += X[idx * D + i] * W[i];\n",
        "        }\n",
        "        y_pred_val += *b; // Use single scalar bias\n",
        "        y_pred[idx] = y_pred_val;\n",
        "        loss[idx] = (y[idx] - y_pred_val) * (y[idx] - y_pred_val); // Squared loss\n",
        "        // Add printf to inspect loss\n",
        "        if (idx == 0 && blockIdx.x == 0) {\n",
        "            printf(\"Epoch %d, Index %d, Loss: %f\\n\", epoch, idx, loss[idx]);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// CUDA kernel to compute gradients\n",
        "__global__ void compute_gradients(float* X, float* loss, float* dW, float* db, int N, int D, int epoch) {\n",
        "    __shared__ float db_shared[BLOCK_SIZE];\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < D) {\n",
        "        float gradW = 0.0f;\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            gradW += X[i * D + idx] * loss[i];\n",
        "        }\n",
        "        dW[idx] = - (2.0f / N) * gradW;\n",
        "        // Add printf to inspect gradients of weights\n",
        "        if (idx == 0 && blockIdx.x == 0) {\n",
        "            printf(\"Epoch %d, Gradient dW[%d]: %f\\n\", epoch, idx, dW[idx]);\n",
        "        }\n",
        "    }\n",
        "    float gradb = 0.0f;\n",
        "    if (idx < N) {\n",
        "        gradb = loss[idx];\n",
        "    }\n",
        "    db_shared[threadIdx.x] = gradb;\n",
        "    __syncthreads();\n",
        "\n",
        "    if (threadIdx.x == 0) {\n",
        "        float sum_db = 0.0f;\n",
        "        for (int i = 0; i < blockDim.x; i++) {\n",
        "            sum_db += db_shared[i];\n",
        "        }\n",
        "        atomicAdd(db, - (2.0f / N) * sum_db);\n",
        "        // Add printf to inspect gradient of bias\n",
        "        if (blockIdx.x == 0) {\n",
        "            printf(\"Epoch %d, Gradient db: %f\\n\", epoch, *db);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// CUDA kernel to update weights using SGD\n",
        "__global__ void update_weights(float* W, float* dW, float* b, float* db, float lr, int D, int epoch) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < D) {\n",
        "        W[idx] -= lr * dW[idx];\n",
        "        // Add printf to inspect updated weights\n",
        "        if (idx == 0 && blockIdx.x == 0) {\n",
        "             printf(\"Epoch %d, Updated W[%d]: %f\\n\", epoch, idx, W[idx]);\n",
        "        }\n",
        "    }\n",
        "    if (idx == 0) {\n",
        "        *b -= lr * (*db);\n",
        "        // Add printf to inspect updated bias\n",
        "         printf(\"Epoch %d, Updated b: %f\\n\", epoch, *b);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Host function to train the model\n",
        "void train_sgd(float* h_X, float* h_y, float* h_W, float* h_b, int N, int D, float lr, int epochs) {\n",
        "    float *d_X, *d_y, *d_W, *d_b, *d_gradW, *d_gradb, *d_loss, *d_y_pred;\n",
        "    cudaMalloc(&d_X, N * D * sizeof(float));\n",
        "    cudaMalloc(&d_y, N * sizeof(float));\n",
        "    cudaMalloc(&d_W, D * sizeof(float));\n",
        "    cudaMalloc(&d_b, sizeof(float));\n",
        "    cudaMalloc(&d_gradW, D * sizeof(float));\n",
        "    cudaMalloc(&d_gradb, sizeof(float));\n",
        "    cudaMalloc(&d_loss, N * sizeof(float));\n",
        "    cudaMalloc(&d_y_pred, N * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_X, h_X, N * D * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_y, h_y, N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_W, h_W, D * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int blocks = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "    int blocks_grad = (D + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    for (int epoch = 0; epoch < epochs; ++epoch) {\n",
        "        compute_loss<<<blocks, BLOCK_SIZE>>>(d_X, d_y, d_W, d_b, d_loss, d_y_pred, N, D, epoch);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        compute_gradients<<<blocks_grad, BLOCK_SIZE>>>(d_X, d_loss, d_gradW, d_gradb, N, D, epoch);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        update_weights<<<blocks_grad, BLOCK_SIZE>>>(d_W, d_gradW, d_b, d_gradb, lr, D, epoch);\n",
        "        cudaDeviceSynchronize();\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(h_W, d_W, D * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(h_b, d_b, sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaFree(d_X);\n",
        "    cudaFree(d_y);\n",
        "    cudaFree(d_W);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_gradW);\n",
        "    cudaFree(d_gradb);\n",
        "    cudaFree(d_loss);\n",
        "    cudaFree(d_y_pred);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 1024;\n",
        "    int D = 10;\n",
        "    float lr = 0.01;\n",
        "    int epochs = 51;\n",
        "\n",
        "    float *h_X = new float[N * D];\n",
        "    float *h_y = new float[N];\n",
        "    float *h_W = new float[D];\n",
        "    float *h_b = new float[1];\n",
        "\n",
        "    srand(42);\n",
        "    for (int i = 0; i < N * D; i++) {\n",
        "        h_X[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "    }\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_y[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "    }\n",
        "    for (int i = 0; i < D; i++) {\n",
        "        h_W[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "    }\n",
        "    *h_b = static_cast<float>(rand()) / RAND_MAX;\n",
        "\n",
        "    train_sgd(h_X, h_y, h_W, h_b, N, D, lr, epochs);\n",
        "\n",
        "    std::cout << \"Trained Weights: \";\n",
        "    for (int i = 0; i < D; i++) std::cout << h_W[i] << \" \";\n",
        "    std::cout << \"\\nTrained Bias: \" << *h_b << std::endl;\n",
        "\n",
        "    delete[] h_X;\n",
        "    delete[] h_y;\n",
        "    delete[] h_W;\n",
        "    delete[] h_b;\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB0OJ7qTNWdJ",
        "outputId": "a134a49d-4d41-4928-9c60-5629baf527d9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sgu.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc sgu.cu -o sgu -gencode arch=compute_75,code=sm_75 -lcublas\n",
        "\n",
        "!/content/sgu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi4GpKdrNeG3",
        "outputId": "848496db-8652-4592-db55-2d177aaa70d3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Index 0, Loss: 2.129148\n",
            "Epoch 0, Gradient dW[0]: -4.974485\n",
            "Epoch 0, Gradient db: -2.261286\n",
            "Epoch 0, Updated W[0]: 0.526596\n",
            "Epoch 0, Updated b: 0.749494\n",
            "Epoch 1, Index 0, Loss: 2.760680\n",
            "Epoch 1, Gradient dW[0]: -6.289608\n",
            "Epoch 1, Gradient db: -5.124981\n",
            "Epoch 1, Updated W[0]: 0.589492\n",
            "Epoch 1, Updated b: 0.800744\n",
            "Epoch 2, Index 0, Loss: 3.767052\n",
            "Epoch 2, Gradient dW[0]: -8.309928\n",
            "Epoch 2, Gradient db: -8.917058\n",
            "Epoch 2, Updated W[0]: 0.672591\n",
            "Epoch 2, Updated b: 0.889915\n",
            "Epoch 3, Index 0, Loss: 5.441096\n",
            "Epoch 3, Gradient dW[0]: -11.567632\n",
            "Epoch 3, Gradient db: -14.210270\n",
            "Epoch 3, Updated W[0]: 0.788267\n",
            "Epoch 3, Updated b: 1.032017\n",
            "Epoch 4, Index 0, Loss: 8.396003\n",
            "Epoch 4, Gradient dW[0]: -17.167271\n",
            "Epoch 4, Gradient db: -22.089975\n",
            "Epoch 4, Updated W[0]: 0.959940\n",
            "Epoch 4, Updated b: 1.252917\n",
            "Epoch 5, Index 0, Loss: 14.052251\n",
            "Epoch 5, Gradient dW[0]: -27.652084\n",
            "Epoch 5, Gradient db: -34.822441\n",
            "Epoch 5, Updated W[0]: 1.236461\n",
            "Epoch 5, Updated b: 1.601142\n",
            "Epoch 6, Index 0, Loss: 26.173527\n",
            "Epoch 6, Gradient dW[0]: -49.738243\n",
            "Epoch 6, Gradient db: -57.793167\n",
            "Epoch 6, Updated W[0]: 1.733843\n",
            "Epoch 6, Updated b: 2.179073\n",
            "Epoch 7, Index 0, Loss: 56.765968\n",
            "Epoch 7, Gradient dW[0]: -104.840439\n",
            "Epoch 7, Gradient db: -106.333649\n",
            "Epoch 7, Updated W[0]: 2.782248\n",
            "Epoch 7, Updated b: 3.242410\n",
            "Epoch 8, Index 0, Loss: 156.029800\n",
            "Epoch 8, Gradient dW[0]: -282.752838\n",
            "Epoch 8, Gradient db: -237.466095\n",
            "Epoch 8, Updated W[0]: 5.609776\n",
            "Epoch 8, Updated b: 5.617071\n",
            "Epoch 9, Index 0, Loss: 645.220642\n",
            "Epoch 9, Gradient dW[0]: -1161.983765\n",
            "Epoch 9, Gradient db: -776.621216\n",
            "Epoch 9, Updated W[0]: 17.229614\n",
            "Epoch 9, Updated b: 13.383283\n",
            "Epoch 10, Index 0, Loss: 5864.486328\n",
            "Epoch 10, Gradient dW[0]: -10644.620117\n",
            "Epoch 10, Gradient db: -5711.487305\n",
            "Epoch 10, Updated W[0]: 123.675812\n",
            "Epoch 10, Updated b: 70.498154\n",
            "Epoch 11, Index 0, Loss: 283169.343750\n",
            "Epoch 11, Gradient dW[0]: -522172.062500\n",
            "Epoch 11, Gradient db: -247413.156250\n",
            "Epoch 11, Updated W[0]: 5345.396484\n",
            "Epoch 11, Updated b: 2544.629639\n",
            "Epoch 12, Index 0, Loss: 509395808.000000\n",
            "Epoch 12, Gradient dW[0]: -948437056.000000\n",
            "Epoch 12, Gradient db: -438851232.000000\n",
            "Epoch 12, Updated W[0]: 9489716.000000\n",
            "Epoch 12, Updated b: 4391057.000000\n",
            "Epoch 13, Index 0, Loss: 1596217616236544.000000\n",
            "Epoch 13, Gradient dW[0]: -2976466828525568.000000\n",
            "Epoch 13, Gradient db: -1376266569646080.000000\n",
            "Epoch 13, Updated W[0]: 29764677009408.000000\n",
            "Epoch 13, Updated b: 13762670100480.000000\n",
            "Epoch 14, Index 0, Loss: 15701313677479833152226590720.000000\n",
            "Epoch 14, Gradient dW[0]: -29279194015288157420711313408.000000\n",
            "Epoch 14, Gradient db: -13538145165629838356245381120.000000\n",
            "Epoch 14, Updated W[0]: 292791942366490863052259328.000000\n",
            "Epoch 14, Updated b: 135381447966949568820543488.000000\n",
            "Epoch 15, Index 0, Loss: inf\n",
            "Epoch 15, Gradient dW[0]: -inf\n",
            "Epoch 15, Gradient db: -inf\n",
            "Epoch 15, Updated W[0]: inf\n",
            "Epoch 15, Updated b: inf\n",
            "Epoch 16, Index 0, Loss: inf\n",
            "Epoch 16, Gradient dW[0]: -inf\n",
            "Epoch 16, Gradient db: -inf\n",
            "Epoch 16, Updated W[0]: inf\n",
            "Epoch 16, Updated b: inf\n",
            "Epoch 17, Index 0, Loss: inf\n",
            "Epoch 17, Gradient dW[0]: -inf\n",
            "Epoch 17, Gradient db: -inf\n",
            "Epoch 17, Updated W[0]: inf\n",
            "Epoch 17, Updated b: inf\n",
            "Epoch 18, Index 0, Loss: inf\n",
            "Epoch 18, Gradient dW[0]: -inf\n",
            "Epoch 18, Gradient db: -inf\n",
            "Epoch 18, Updated W[0]: inf\n",
            "Epoch 18, Updated b: inf\n",
            "Epoch 19, Index 0, Loss: inf\n",
            "Epoch 19, Gradient dW[0]: -inf\n",
            "Epoch 19, Gradient db: -inf\n",
            "Epoch 19, Updated W[0]: inf\n",
            "Epoch 19, Updated b: inf\n",
            "Epoch 20, Index 0, Loss: inf\n",
            "Epoch 20, Gradient dW[0]: -inf\n",
            "Epoch 20, Gradient db: -inf\n",
            "Epoch 20, Updated W[0]: inf\n",
            "Epoch 20, Updated b: inf\n",
            "Epoch 21, Index 0, Loss: inf\n",
            "Epoch 21, Gradient dW[0]: -inf\n",
            "Epoch 21, Gradient db: -inf\n",
            "Epoch 21, Updated W[0]: inf\n",
            "Epoch 21, Updated b: inf\n",
            "Epoch 22, Index 0, Loss: inf\n",
            "Epoch 22, Gradient dW[0]: -inf\n",
            "Epoch 22, Gradient db: -inf\n",
            "Epoch 22, Updated W[0]: inf\n",
            "Epoch 22, Updated b: inf\n",
            "Epoch 23, Index 0, Loss: inf\n",
            "Epoch 23, Gradient dW[0]: -inf\n",
            "Epoch 23, Gradient db: -inf\n",
            "Epoch 23, Updated W[0]: inf\n",
            "Epoch 23, Updated b: inf\n",
            "Epoch 24, Index 0, Loss: inf\n",
            "Epoch 24, Gradient dW[0]: -inf\n",
            "Epoch 24, Gradient db: -inf\n",
            "Epoch 24, Updated W[0]: inf\n",
            "Epoch 24, Updated b: inf\n",
            "Epoch 25, Index 0, Loss: inf\n",
            "Epoch 25, Gradient dW[0]: -inf\n",
            "Epoch 25, Gradient db: -inf\n",
            "Epoch 25, Updated W[0]: inf\n",
            "Epoch 25, Updated b: inf\n",
            "Epoch 26, Index 0, Loss: inf\n",
            "Epoch 26, Gradient dW[0]: -inf\n",
            "Epoch 26, Gradient db: -inf\n",
            "Epoch 26, Updated W[0]: inf\n",
            "Epoch 26, Updated b: inf\n",
            "Epoch 27, Index 0, Loss: inf\n",
            "Epoch 27, Gradient dW[0]: -inf\n",
            "Epoch 27, Gradient db: -inf\n",
            "Epoch 27, Updated W[0]: inf\n",
            "Epoch 27, Updated b: inf\n",
            "Epoch 28, Index 0, Loss: inf\n",
            "Epoch 28, Gradient dW[0]: -inf\n",
            "Epoch 28, Gradient db: -inf\n",
            "Epoch 28, Updated W[0]: inf\n",
            "Epoch 28, Updated b: inf\n",
            "Epoch 29, Index 0, Loss: inf\n",
            "Epoch 29, Gradient dW[0]: -inf\n",
            "Epoch 29, Gradient db: -inf\n",
            "Epoch 29, Updated W[0]: inf\n",
            "Epoch 29, Updated b: inf\n",
            "Epoch 30, Index 0, Loss: inf\n",
            "Epoch 30, Gradient dW[0]: -inf\n",
            "Epoch 30, Gradient db: -inf\n",
            "Epoch 30, Updated W[0]: inf\n",
            "Epoch 30, Updated b: inf\n",
            "Epoch 31, Index 0, Loss: inf\n",
            "Epoch 31, Gradient dW[0]: -inf\n",
            "Epoch 31, Gradient db: -inf\n",
            "Epoch 31, Updated W[0]: inf\n",
            "Epoch 31, Updated b: inf\n",
            "Epoch 32, Index 0, Loss: inf\n",
            "Epoch 32, Gradient dW[0]: -inf\n",
            "Epoch 32, Gradient db: -inf\n",
            "Epoch 32, Updated W[0]: inf\n",
            "Epoch 32, Updated b: inf\n",
            "Epoch 33, Index 0, Loss: inf\n",
            "Epoch 33, Gradient dW[0]: -inf\n",
            "Epoch 33, Gradient db: -inf\n",
            "Epoch 33, Updated W[0]: inf\n",
            "Epoch 33, Updated b: inf\n",
            "Epoch 34, Index 0, Loss: inf\n",
            "Epoch 34, Gradient dW[0]: -inf\n",
            "Epoch 34, Gradient db: -inf\n",
            "Epoch 34, Updated W[0]: inf\n",
            "Epoch 34, Updated b: inf\n",
            "Epoch 35, Index 0, Loss: inf\n",
            "Epoch 35, Gradient dW[0]: -inf\n",
            "Epoch 35, Gradient db: -inf\n",
            "Epoch 35, Updated W[0]: inf\n",
            "Epoch 35, Updated b: inf\n",
            "Epoch 36, Index 0, Loss: inf\n",
            "Epoch 36, Gradient dW[0]: -inf\n",
            "Epoch 36, Gradient db: -inf\n",
            "Epoch 36, Updated W[0]: inf\n",
            "Epoch 36, Updated b: inf\n",
            "Epoch 37, Index 0, Loss: inf\n",
            "Epoch 37, Gradient dW[0]: -inf\n",
            "Epoch 37, Gradient db: -inf\n",
            "Epoch 37, Updated W[0]: inf\n",
            "Epoch 37, Updated b: inf\n",
            "Epoch 38, Index 0, Loss: inf\n",
            "Epoch 38, Gradient dW[0]: -inf\n",
            "Epoch 38, Gradient db: -inf\n",
            "Epoch 38, Updated W[0]: inf\n",
            "Epoch 38, Updated b: inf\n",
            "Epoch 39, Index 0, Loss: inf\n",
            "Epoch 39, Gradient dW[0]: -inf\n",
            "Epoch 39, Gradient db: -inf\n",
            "Epoch 39, Updated W[0]: inf\n",
            "Epoch 39, Updated b: inf\n",
            "Epoch 40, Index 0, Loss: inf\n",
            "Epoch 40, Gradient dW[0]: -inf\n",
            "Epoch 40, Gradient db: -inf\n",
            "Epoch 40, Updated W[0]: inf\n",
            "Epoch 40, Updated b: inf\n",
            "Epoch 41, Index 0, Loss: inf\n",
            "Epoch 41, Gradient dW[0]: -inf\n",
            "Epoch 41, Gradient db: -inf\n",
            "Epoch 41, Updated W[0]: inf\n",
            "Epoch 41, Updated b: inf\n",
            "Epoch 42, Index 0, Loss: inf\n",
            "Epoch 42, Gradient dW[0]: -inf\n",
            "Epoch 42, Gradient db: -inf\n",
            "Epoch 42, Updated W[0]: inf\n",
            "Epoch 42, Updated b: inf\n",
            "Epoch 43, Index 0, Loss: inf\n",
            "Epoch 43, Gradient dW[0]: -inf\n",
            "Epoch 43, Gradient db: -inf\n",
            "Epoch 43, Updated W[0]: inf\n",
            "Epoch 43, Updated b: inf\n",
            "Epoch 44, Index 0, Loss: inf\n",
            "Epoch 44, Gradient dW[0]: -inf\n",
            "Epoch 44, Gradient db: -inf\n",
            "Epoch 44, Updated W[0]: inf\n",
            "Epoch 44, Updated b: inf\n",
            "Epoch 45, Index 0, Loss: inf\n",
            "Epoch 45, Gradient dW[0]: -inf\n",
            "Epoch 45, Gradient db: -inf\n",
            "Epoch 45, Updated W[0]: inf\n",
            "Epoch 45, Updated b: inf\n",
            "Epoch 46, Index 0, Loss: inf\n",
            "Epoch 46, Gradient dW[0]: -inf\n",
            "Epoch 46, Gradient db: -inf\n",
            "Epoch 46, Updated W[0]: inf\n",
            "Epoch 46, Updated b: inf\n",
            "Epoch 47, Index 0, Loss: inf\n",
            "Epoch 47, Gradient dW[0]: -inf\n",
            "Epoch 47, Gradient db: -inf\n",
            "Epoch 47, Updated W[0]: inf\n",
            "Epoch 47, Updated b: inf\n",
            "Epoch 48, Index 0, Loss: inf\n",
            "Epoch 48, Gradient dW[0]: -inf\n",
            "Epoch 48, Gradient db: -inf\n",
            "Epoch 48, Updated W[0]: inf\n",
            "Epoch 48, Updated b: inf\n",
            "Epoch 49, Index 0, Loss: inf\n",
            "Epoch 49, Gradient dW[0]: -inf\n",
            "Epoch 49, Gradient db: -inf\n",
            "Epoch 49, Updated W[0]: inf\n",
            "Epoch 49, Updated b: inf\n",
            "Epoch 50, Index 0, Loss: inf\n",
            "Epoch 50, Gradient dW[0]: -inf\n",
            "Epoch 50, Gradient db: -inf\n",
            "Epoch 50, Updated W[0]: inf\n",
            "Epoch 50, Updated b: inf\n",
            "Trained Weights: inf inf inf inf inf inf inf inf inf inf \n",
            "Trained Bias: inf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fU3llumxNnYj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}