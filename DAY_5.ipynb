{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAgWpxgX5yvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "444bcade-262f-4ebb-c406-55ccdd1e9804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing MatricAdd.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile  MatricAdd.cu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile  LayerNorm.cu\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void LayerNorm(const float* A, float* B, int rows, int cols) {\n",
        "    // Calculate row index\n",
        "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < rows) {\n",
        "        // Use shared memory for row-wise computation\n",
        "        extern __shared__ float shared[];\n",
        "        float* row_data = shared;\n",
        "\n",
        "        // Copy row data to shared memory\n",
        "        for (int col = threadIdx.y; col < cols; col += blockDim.y) {\n",
        "            row_data[col] = A[row * cols + col];\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        // Compute mean\n",
        "        float mean = 0.0f;\n",
        "        for (int col = 0; col < cols; col++) {\n",
        "            mean += row_data[col];\n",
        "        }\n",
        "        mean /= cols;\n",
        "\n",
        "        // Compute variance\n",
        "        float variance = 0.0f;\n",
        "        for (int col = 0; col < cols; col++) {\n",
        "            variance += (row_data[col] - mean) * (row_data[col] - mean);\n",
        "        }\n",
        "        variance /= cols;\n",
        "        float stddev = sqrtf(variance + 1e-7); // Add epsilon for numerical stability\n",
        "\n",
        "        // Normalize\n",
        "        for (int col = threadIdx.y; col < cols; col += blockDim.y) {\n",
        "            B[row * cols + col] = (row_data[col] - mean) / stddev;\n",
        "        }\n",
        "\n",
        "        // Print statements for debugging\n",
        "        if (threadIdx.y == 0) {\n",
        "            printf(\"Row: %d, Mean: %f, Stddev: %f\\n\", row, mean, stddev);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int rows = 10, cols = 10;\n",
        "    float *A, *B;\n",
        "\n",
        "    // Allocate host memory\n",
        "    A = (float*)malloc(rows * cols * sizeof(float));\n",
        "    B = (float*)malloc(rows * cols * sizeof(float));\n",
        "\n",
        "    // Initialize input matrix\n",
        "    for (int i = 0; i < rows; i++) {\n",
        "        for (int j = 0; j < cols; j++) {\n",
        "            A[i * cols + j] = static_cast<float>(rand()) / RAND_MAX;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Allocate device memory\n",
        "    float *d_a, *d_b;\n",
        "    cudaMalloc(&d_a, rows * cols * sizeof(float));\n",
        "    cudaMalloc(&d_b, rows * cols * sizeof(float));\n",
        "\n",
        "    // Copy data to device\n",
        "    cudaMemcpy(d_a, A, rows * cols * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel\n",
        "    int blocksize = 256;\n",
        "    int gridsize = (rows + blocksize - 1) / blocksize;\n",
        "    size_t shared_memory_size = cols * sizeof(float);\n",
        "    LayerNorm<<<gridsize, blocksize, shared_memory_size>>>(d_a, d_b, rows, cols);\n",
        "\n",
        "    // Synchronize device\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(B, d_b, rows * cols * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print results\n",
        "    printf(\"A:\\n\");\n",
        "    for (int i = 0; i < rows; i++) {\n",
        "        for (int j = 0; j < cols; j++) {\n",
        "            printf(\"%.2f \", A[i * cols + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"\\nB:\\n\");\n",
        "    for (int i = 0; i < rows; i++) {\n",
        "        for (int j = 0; j < cols; j++) {\n",
        "            printf(\"%.2f \", B[i * cols + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // Free memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    free(A);\n",
        "    free(B);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_aQJ3-o6PGr",
        "outputId": "a8b1c66a-0dbc-4fd4-d4cc-b5d7bb1c109d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing LayerNorm.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!nvcc LayerNorm.cu -o LayerNorm -gencode arch=compute_75,code=sm_75\n",
        "\n",
        "!./LayerNorm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk0EkceP6jeU",
        "outputId": "7f5da24a-7557-4add-b528-4b79c480ed2e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A:\n",
            "0.84 0.39 0.78 0.80 0.91 0.20 0.34 0.77 0.28 0.55 \n",
            "0.48 0.63 0.36 0.51 0.95 0.92 0.64 0.72 0.14 0.61 \n",
            "0.02 0.24 0.14 0.80 0.16 0.40 0.13 0.11 1.00 0.22 \n",
            "0.51 0.84 0.61 0.30 0.64 0.52 0.49 0.97 0.29 0.77 \n",
            "0.53 0.77 0.40 0.89 0.28 0.35 0.81 0.92 0.07 0.95 \n",
            "0.53 0.09 0.19 0.66 0.89 0.35 0.06 0.02 0.46 0.06 \n",
            "0.24 0.97 0.90 0.85 0.27 0.54 0.38 0.76 0.51 0.67 \n",
            "0.53 0.04 0.44 0.93 0.93 0.72 0.28 0.74 0.64 0.35 \n",
            "0.69 0.17 0.44 0.88 0.83 0.33 0.23 0.89 0.35 0.69 \n",
            "0.96 0.59 0.66 0.86 0.44 0.92 0.40 0.81 0.68 0.91 \n",
            "\n",
            "B:\n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n",
            "0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f30da915"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0adb1152"
      },
      "source": [],
      "execution_count": 3,
      "outputs": []
    }
  ]
}